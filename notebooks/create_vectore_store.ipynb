{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csvPath=\"../data/job profiles/2025-02-07_profiles.csv\"\n",
    "df=pd.read_csv(csvPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CREATES VECTORSTORE WITH DOCUMENTS PER SECTION\n",
    "import json\n",
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "import json\n",
    "\n",
    "def process_job_profiles(csv_path: str) -> List[Document]:\n",
    "    \"\"\"Process job profiles CSV with pandas and JSON handling\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    documents = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        # Parse JSON fields if necessary\n",
    "        role = json.loads(row['role']) if pd.notna(row.get('role')) else None\n",
    "        role_type = json.loads(row['role_type']) if pd.notna(row.get('role_type')) else None\n",
    "        scopes = json.loads(row['scopes']) if pd.notna(row.get('scopes')) else None\n",
    "\n",
    "        # Base metadata for all documents\n",
    "        base_metadata = {\n",
    "            \"title\": row.get(\"title\", \"\"),\n",
    "            \"number\": row.get(\"number\", \"\"),\n",
    "            \"type\": row.get(\"type\", \"\"),\n",
    "            \"context\": row.get(\"context\", \"\"),\n",
    "            \"views\": row.get(\"views\", \"\"),\n",
    "            \"role\": f\"{role['name']}\" if role else \"\",\n",
    "            \"role_type\": f\"{role_type['name']}\" if role_type else \"\",\n",
    "            \"scopes\": \", \".join([item[\"name\"] for item in scopes]) if scopes else \"\",\n",
    "            \"created_at\": row.get(\"created_at\", \"\"),\n",
    "            \"updated_at\": row.get(\"updated_at\", \"\"),\n",
    "            \"row_index\": idx,\n",
    "        }\n",
    "\n",
    "        # Process classifications and organizations\n",
    "        classifications = \"\"\n",
    "        organizations = \"\"\n",
    "        for field in [\"classifications\", \"organizations\"]:\n",
    "            if pd.notna(row.get(field)):\n",
    "                try:\n",
    "                    items = json.loads(row[field])\n",
    "                    if field==\"classifications\":\n",
    "                        tags = [f\"{item['name']}\" for item in items]\n",
    "                    else:\n",
    "                        tags = [f\"{item['name']} ({item['code']})\" for item in items]\n",
    "                    \n",
    "                    base_metadata[field] = \", \".join(tags)\n",
    "                    if field == \"classifications\":\n",
    "                        classifications = \", \".join(tags)\n",
    "                    else:\n",
    "                        organizations = \", \".join(tags)\n",
    "                except json.JSONDecodeError:\n",
    "                    pass\n",
    "\n",
    "        # Add a prefix to provide context for each document\n",
    "        prefix_content = f\"\"\"Job Profile Title: {base_metadata['title']}\n",
    "Classifications: {classifications}\n",
    "Organizations: {organizations}\n",
    "\"\"\"\n",
    "\n",
    "        # Define array fields with semantic section titles\n",
    "        array_fields = {\n",
    "            \"behavioural_competencies\": (\"Behavioural Competencies\", (\"name\", \"description\")),\n",
    "            \"education\": (\"Education\", (\"text\",)),\n",
    "            \"job_experience\": (\"Job Experience\", (\"text\",)),\n",
    "            \"professional_registration_requirements\": (\"Professional Registration Requirements\", (\"text\",)),\n",
    "            \"preferences\": (\"Preferences\", (\"text\",)),\n",
    "            \"knowledge_skills_abilities\": (\"Knowledge, Skills, and Abilities\", (\"text\",)),\n",
    "            \"willingness_statements\": (\"Willingness Statements\", (\"text\",)),\n",
    "            \"security_screenings\": (\"Security Screenings\", (\"text\",)),\n",
    "            \"accountabilities\": (\"Accountabilities\", (\"text\",)),\n",
    "        }\n",
    "\n",
    "        # Process each array field\n",
    "        for field, (section_title, attributes) in array_fields.items():\n",
    "            if pd.notna(row.get(field)):\n",
    "                try:\n",
    "                    items = json.loads(row[field])\n",
    "                    section_content = prefix_content + f\"\\nSection: {section_title}\\n\"\n",
    "                    \n",
    "                    # Collect all items for this section\n",
    "                    if field == \"behavioural_competencies\":\n",
    "                        section_items = [f\"• {item['name']}: {item['description']}\" for item in items]\n",
    "                    else:\n",
    "                        section_items = [f\"• {item[attributes[0]]}\" for item in items]\n",
    "                    \n",
    "                    # Join all items with newlines\n",
    "                    section_content += \"\\n\".join(section_items)\n",
    "                    \n",
    "                    # Create one document for the entire section\n",
    "                    doc = Document(\n",
    "                        page_content=section_content,\n",
    "                        metadata={\n",
    "                            **base_metadata,\n",
    "                            \"section\": section_title\n",
    "                        }\n",
    "                    )\n",
    "                    documents.append(doc)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Row {idx}: Invalid JSON in {field} - {str(e)}\")\n",
    "\n",
    "    return documents\n",
    "\n",
    "\n",
    "documents=process_job_profiles(csvPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROFILE PER DOCUMENT\n",
    "def process_job_profiles(csv_path: str) -> List[Document]:\n",
    "    \"\"\"Process job profiles CSV and create one document per job profile\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    documents = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        # Parse JSON fields\n",
    "        # json_fields = ['role', 'role_type', 'scopes', 'classifications', 'organizations']\n",
    "        parsed_data = {}\n",
    "        \n",
    "        # for field in json_fields:\n",
    "        #     if pd.notna(row.get(field)):\n",
    "        #         try:\n",
    "        #             data = json.loads(row[field])\n",
    "        #             if field in ['classifications', 'organizations']:\n",
    "        #                 parsed_data[field] = \", \".join([f\"{item['name']}\" + (f\" ({item['code']})\" if field == 'organizations' else \"\") for item in data])\n",
    "        #             elif field == 'scopes':\n",
    "        #                 parsed_data[field] = \", \".join([item[\"name\"] for item in data])\n",
    "        #             else:\n",
    "        #                 parsed_data[field] = data['name']\n",
    "        #         except json.JSONDecodeError:\n",
    "        #             parsed_data[field] = \"\"\n",
    "\n",
    "        # Process classifications\n",
    "        if pd.notna(row.get('classifications')):\n",
    "            try:\n",
    "                classifications_data = json.loads(row['classifications'])\n",
    "                # Create a set to remove duplicates\n",
    "                classification_names = set(item['name'] for item in classifications_data)\n",
    "                parsed_data['classifications'] = \", \".join(sorted(classification_names))\n",
    "            except json.JSONDecodeError:\n",
    "                parsed_data['classifications'] = \"\"\n",
    "\n",
    "        # Process organizations\n",
    "        if pd.notna(row.get('organizations')):\n",
    "            try:\n",
    "                organizations_data = json.loads(row['organizations'])\n",
    "                # Create a set of tuples (name, code) to remove duplicates\n",
    "                org_items = set((item['name'], item['code']) for item in organizations_data)\n",
    "                # Join with formatting\n",
    "                parsed_data['organizations'] = \", \".join(\n",
    "                    f\"{name} ({code})\" for name, code in sorted(org_items)\n",
    "                )\n",
    "            except json.JSONDecodeError:\n",
    "                parsed_data['organizations'] = \"\"\n",
    "\n",
    "        # Process other JSON fields (role, role_type, scopes)\n",
    "        for field in ['role', 'role_type', 'scopes']:\n",
    "            if pd.notna(row.get(field)):\n",
    "                try:\n",
    "                    data = json.loads(row[field])\n",
    "                    if field == 'scopes':\n",
    "                        scope_names = set(item[\"name\"] for item in data)  # Remove duplicates\n",
    "                        parsed_data[field] = \", \".join(sorted(scope_names))\n",
    "                    else:\n",
    "                        parsed_data[field] = data['name']\n",
    "                except json.JSONDecodeError:\n",
    "                    parsed_data[field] = \"\"\n",
    "\n",
    "        # Create metadata\n",
    "        metadata = {\n",
    "            \"title\": row.get(\"title\", \"\"),\n",
    "            \"number\": row.get(\"number\", \"\"),\n",
    "            \"type\": row.get(\"type\", \"\"),\n",
    "            \"context\": row.get(\"context\", \"\"),\n",
    "            \"views\": row.get(\"views\", \"\"),\n",
    "            \"role\": parsed_data.get('role', \"\"),\n",
    "            \"role_type\": parsed_data.get('role_type', \"\"),\n",
    "            \"scopes\": parsed_data.get('scopes', \"\"),\n",
    "            \"classifications\": parsed_data.get('classifications', \"\"),\n",
    "            \"organizations\": parsed_data.get('organizations', \"\"),\n",
    "            \"created_at\": row.get(\"created_at\", \"\"),\n",
    "            \"updated_at\": row.get(\"updated_at\", \"\"),\n",
    "            \"row_index\": idx,\n",
    "        }\n",
    "\n",
    "        # Build content sections\n",
    "        content_sections = [\n",
    "            f\"Job Profile Title: {metadata['title']}\",\n",
    "            f\"Classifications: {metadata['classifications']}\",\n",
    "            f\"Organizations: {metadata['organizations']}\"\n",
    "        ]\n",
    "\n",
    "        # Array fields to process\n",
    "        array_fields = {\n",
    "            \"behavioural_competencies\": \"Behavioural Competencies\",\n",
    "            \"education\": \"Education\",\n",
    "            \"job_experience\": \"Job Experience\",\n",
    "            \"professional_registration_requirements\": \"Professional Registration Requirements\",\n",
    "            \"preferences\": \"Preferences\",\n",
    "            \"knowledge_skills_abilities\": \"Knowledge, Skills, and Abilities\",\n",
    "            \"willingness_statements\": \"Willingness Statements\",\n",
    "            \"security_screenings\": \"Security Screenings\",\n",
    "            \"accountabilities\": \"Accountabilities\",\n",
    "        }\n",
    "\n",
    "        # Process each section\n",
    "        for field, section_title in array_fields.items():\n",
    "            if pd.notna(row.get(field)):\n",
    "                try:\n",
    "                    items = json.loads(row[field])\n",
    "                    if(len(items)==0):\n",
    "                        continue\n",
    "                    content_sections.append(f'\\n{section_title} for the \"{row.get(\"title\", \"\")}\" job profile:')\n",
    "                    \n",
    "                    if field == \"behavioural_competencies\":\n",
    "                        \n",
    "                        section_items = [f\"• {item['name']}: {item['description']}\" for item in items]\n",
    "                    else:\n",
    "                        section_items = [f\"• {item['text']}\" for item in items]\n",
    "                    \n",
    "                    content_sections.extend(section_items)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "\n",
    "        # Create one document with all content\n",
    "        doc = Document(\n",
    "            page_content=\"\\n\".join(content_sections),\n",
    "            metadata=metadata\n",
    "        )\n",
    "        documents.append(doc)\n",
    "\n",
    "    return documents\n",
    "documents=process_job_profiles(csvPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def dump_documents_to_console(documents, limit=10):\n",
    "#     \"\"\"Prints a preview of the generated documents to the console.\"\"\"\n",
    "#     print(f\"Total documents generated: {len(documents)}\\n\")\n",
    "#     for i, doc in enumerate(documents[:limit]):  # Limit the number of documents printed\n",
    "#         print(f\"Document {i + 1}:\")\n",
    "#         print(\"-\" * 50)\n",
    "#         print(f\"Content:\\n{doc.page_content}\\n\")\n",
    "#         print(f\"Metadata:\\n{json.dumps(doc.metadata, indent=4)}\")\n",
    "#         print(\"=\" * 50)\n",
    "\n",
    "# # Dump documents to console\n",
    "# dump_documents_to_console(documents, limit=100)  # Adjust limit as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents adding: 2136\n",
      "creating vector store..\n",
      "Processing batch 1, size: 100\n",
      "Current document count: 100\n",
      "Processing batch 2, size: 100\n",
      "Current document count: 200\n",
      "Processing batch 3, size: 100\n",
      "Current document count: 300\n",
      "Processing batch 4, size: 100\n",
      "Current document count: 400\n",
      "Processing batch 5, size: 100\n",
      "Current document count: 500\n",
      "Processing batch 6, size: 100\n",
      "Current document count: 600\n",
      "Processing batch 7, size: 100\n",
      "Current document count: 700\n",
      "Processing batch 8, size: 100\n",
      "Current document count: 800\n",
      "Processing batch 9, size: 100\n",
      "Current document count: 900\n",
      "Processing batch 10, size: 100\n",
      "Current document count: 1000\n",
      "Processing batch 11, size: 100\n",
      "Current document count: 1100\n",
      "Processing batch 12, size: 100\n",
      "Current document count: 1200\n",
      "Processing batch 13, size: 100\n",
      "Current document count: 1300\n",
      "Processing batch 14, size: 100\n",
      "Current document count: 1400\n",
      "Processing batch 15, size: 100\n",
      "Current document count: 1500\n",
      "Processing batch 16, size: 100\n",
      "Current document count: 1600\n",
      "Processing batch 17, size: 100\n",
      "Current document count: 1700\n",
      "Processing batch 18, size: 100\n",
      "Current document count: 1800\n",
      "Processing batch 19, size: 100\n",
      "Current document count: 1900\n",
      "Processing batch 20, size: 100\n",
      "Current document count: 2000\n",
      "Processing batch 21, size: 100\n",
      "Current document count: 2100\n",
      "Processing batch 22, size: 36\n",
      "Current document count: 2136\n",
      "Collection count: 2136\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import CSVLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import chromadb\n",
    "\n",
    "client = chromadb.PersistentClient(\"../job_profiles_db2\")\n",
    "collection = client.get_or_create_collection(\"job_profiles\",metadata={\"hnsw:batch_size\":10000})\n",
    "\n",
    "def create_vectorstore_with_batching(documents, batch_size=100):  # Reduced batch size\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"thenlper/gte-small\")\n",
    "    \n",
    "    vectorstore = Chroma(\n",
    "        # persist_directory=\"job_profiles_db\",\n",
    "        client=client,\n",
    "        embedding_function=embeddings,\n",
    "        collection_name=\"job_profiles\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        for i in range(0, len(documents), batch_size):\n",
    "            batch = documents[i:i + batch_size]\n",
    "            print(f\"Processing batch {i//batch_size + 1}, size: {len(batch)}\")\n",
    "            \n",
    "            vectorstore.add_documents(documents=batch)\n",
    "            \n",
    "            # Add verification step\n",
    "            current_count = vectorstore._collection.count()\n",
    "            print(f\"Current document count: {current_count}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error during processing: {str(e)}\")\n",
    "        \n",
    "    return vectorstore\n",
    "\n",
    "# loader = CSVLoader(file_path=\"../data/job profiles/2025-02-07_profiles.csv\", content_columns=[\"title\", \"overview\"], encoding=\"utf-8-sig\")\n",
    "# documents = loader.load()\n",
    "\n",
    "\n",
    "# text_splitter = RecursiveCharacterTextSplitter(\n",
    "#     chunk_size=1000,\n",
    "#     chunk_overlap=200,\n",
    "#     separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "# )\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    AutoTokenizer.from_pretrained(\"thenlper/gte-small\"),\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=50,\n",
    "    add_start_index=True,\n",
    "    strip_whitespace=True\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "# chunks = documents\n",
    "print(f\"Documents adding: {len(chunks)}\")\n",
    "print('creating vector store..')\n",
    "vectorstore = create_vectorstore_with_batching(chunks)\n",
    "\n",
    "print(f\"Collection count: {vectorstore._collection.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "job-profiles-rag-1qLAWhuo-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
