{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "csvPath=\"../data/job profiles/2025-02-07_profiles.csv\"\n",
    "df=pd.read_csv(csvPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from notebooks.utils import get_job_profile_documents_per_section\n",
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "documents=get_job_profile_documents_per_section(csvPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROFILE PER DOCUMENT\n",
    "\n",
    "from notebooks.utils import get_job_profile_documents\n",
    "documents=get_job_profile_documents(csvPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def dump_documents_to_console(documents, limit=10):\n",
    "#     \"\"\"Prints a preview of the generated documents to the console.\"\"\"\n",
    "#     print(f\"Total documents generated: {len(documents)}\\n\")\n",
    "#     for i, doc in enumerate(documents[:limit]):  # Limit the number of documents printed\n",
    "#         print(f\"Document {i + 1}:\")\n",
    "#         print(\"-\" * 50)\n",
    "#         print(f\"Content:\\n{doc.page_content}\\n\")\n",
    "#         print(f\"Metadata:\\n{json.dumps(doc.metadata, indent=4)}\")\n",
    "#         print(\"=\" * 50)\n",
    "\n",
    "# # Dump documents to console\n",
    "# dump_documents_to_console(documents, limit=100)  # Adjust limit as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents adding: 2136\n",
      "creating vector store..\n",
      "Processing batch 1, size: 100\n",
      "Current document count: 100\n",
      "Processing batch 2, size: 100\n",
      "Current document count: 200\n",
      "Processing batch 3, size: 100\n",
      "Current document count: 300\n",
      "Processing batch 4, size: 100\n",
      "Current document count: 400\n",
      "Processing batch 5, size: 100\n",
      "Current document count: 500\n",
      "Processing batch 6, size: 100\n",
      "Current document count: 600\n",
      "Processing batch 7, size: 100\n",
      "Current document count: 700\n",
      "Processing batch 8, size: 100\n",
      "Current document count: 800\n",
      "Processing batch 9, size: 100\n",
      "Current document count: 900\n",
      "Processing batch 10, size: 100\n",
      "Current document count: 1000\n",
      "Processing batch 11, size: 100\n",
      "Current document count: 1100\n",
      "Processing batch 12, size: 100\n",
      "Current document count: 1200\n",
      "Processing batch 13, size: 100\n",
      "Current document count: 1300\n",
      "Processing batch 14, size: 100\n",
      "Current document count: 1400\n",
      "Processing batch 15, size: 100\n",
      "Current document count: 1500\n",
      "Processing batch 16, size: 100\n",
      "Current document count: 1600\n",
      "Processing batch 17, size: 100\n",
      "Current document count: 1700\n",
      "Processing batch 18, size: 100\n",
      "Current document count: 1800\n",
      "Processing batch 19, size: 100\n",
      "Current document count: 1900\n",
      "Processing batch 20, size: 100\n",
      "Current document count: 2000\n",
      "Processing batch 21, size: 100\n",
      "Current document count: 2100\n",
      "Processing batch 22, size: 36\n",
      "Current document count: 2136\n",
      "Collection count: 2136\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import CSVLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import chromadb\n",
    "\n",
    "client = chromadb.PersistentClient(\"../job_profiles_db2\")\n",
    "collection = client.get_or_create_collection(\"job_profiles\",metadata={\"hnsw:batch_size\":10000})\n",
    "\n",
    "def create_vectorstore_with_batching(documents, batch_size=100):  # Reduced batch size\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"thenlper/gte-small\")\n",
    "    \n",
    "    vectorstore = Chroma(\n",
    "        # persist_directory=\"job_profiles_db\",\n",
    "        client=client,\n",
    "        embedding_function=embeddings,\n",
    "        collection_name=\"job_profiles\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        for i in range(0, len(documents), batch_size):\n",
    "            batch = documents[i:i + batch_size]\n",
    "            print(f\"Processing batch {i//batch_size + 1}, size: {len(batch)}\")\n",
    "            \n",
    "            vectorstore.add_documents(documents=batch)\n",
    "            \n",
    "            # Add verification step\n",
    "            current_count = vectorstore._collection.count()\n",
    "            print(f\"Current document count: {current_count}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error during processing: {str(e)}\")\n",
    "        \n",
    "    return vectorstore\n",
    "\n",
    "# loader = CSVLoader(file_path=\"../data/job profiles/2025-02-07_profiles.csv\", content_columns=[\"title\", \"overview\"], encoding=\"utf-8-sig\")\n",
    "# documents = loader.load()\n",
    "\n",
    "\n",
    "# text_splitter = RecursiveCharacterTextSplitter(\n",
    "#     chunk_size=1000,\n",
    "#     chunk_overlap=200,\n",
    "#     separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "# )\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    AutoTokenizer.from_pretrained(\"thenlper/gte-small\"),\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=50,\n",
    "    add_start_index=True,\n",
    "    strip_whitespace=True\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "# chunks = documents\n",
    "print(f\"Documents adding: {len(chunks)}\")\n",
    "print('creating vector store..')\n",
    "vectorstore = create_vectorstore_with_batching(chunks)\n",
    "\n",
    "print(f\"Collection count: {vectorstore._collection.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
