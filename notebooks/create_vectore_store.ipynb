{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csvPath=\"../data/job profiles/2025-02-07_profiles.csv\"\n",
    "df=pd.read_csv(csvPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from notebooks.utils import get_job_profile_documents\n",
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "documents=get_job_profile_documents(csvPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROFILE PER DOCUMENT\n",
    "def process_job_profiles(csv_path: str) -> List[Document]:\n",
    "    \"\"\"Process job profiles CSV and create one document per job profile\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    documents = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        # Parse JSON fields\n",
    "        # json_fields = ['role', 'role_type', 'scopes', 'classifications', 'organizations']\n",
    "        parsed_data = {}\n",
    "        \n",
    "        # for field in json_fields:\n",
    "        #     if pd.notna(row.get(field)):\n",
    "        #         try:\n",
    "        #             data = json.loads(row[field])\n",
    "        #             if field in ['classifications', 'organizations']:\n",
    "        #                 parsed_data[field] = \", \".join([f\"{item['name']}\" + (f\" ({item['code']})\" if field == 'organizations' else \"\") for item in data])\n",
    "        #             elif field == 'scopes':\n",
    "        #                 parsed_data[field] = \", \".join([item[\"name\"] for item in data])\n",
    "        #             else:\n",
    "        #                 parsed_data[field] = data['name']\n",
    "        #         except json.JSONDecodeError:\n",
    "        #             parsed_data[field] = \"\"\n",
    "\n",
    "        # Process classifications\n",
    "        if pd.notna(row.get('classifications')):\n",
    "            try:\n",
    "                classifications_data = json.loads(row['classifications'])\n",
    "                # Create a set to remove duplicates\n",
    "                classification_names = set(item['name'] for item in classifications_data)\n",
    "                parsed_data['classifications'] = \", \".join(sorted(classification_names))\n",
    "            except json.JSONDecodeError:\n",
    "                parsed_data['classifications'] = \"\"\n",
    "\n",
    "        # Process organizations\n",
    "        if pd.notna(row.get('organizations')):\n",
    "            try:\n",
    "                organizations_data = json.loads(row['organizations'])\n",
    "                # Create a set of tuples (name, code) to remove duplicates\n",
    "                org_items = set((item['name'], item['code']) for item in organizations_data)\n",
    "                # Join with formatting\n",
    "                parsed_data['organizations'] = \", \".join(\n",
    "                    f\"{name} ({code})\" for name, code in sorted(org_items)\n",
    "                )\n",
    "            except json.JSONDecodeError:\n",
    "                parsed_data['organizations'] = \"\"\n",
    "\n",
    "        # Process other JSON fields (role, role_type, scopes)\n",
    "        for field in ['role', 'role_type', 'scopes']:\n",
    "            if pd.notna(row.get(field)):\n",
    "                try:\n",
    "                    data = json.loads(row[field])\n",
    "                    if field == 'scopes':\n",
    "                        scope_names = set(item[\"name\"] for item in data)  # Remove duplicates\n",
    "                        parsed_data[field] = \", \".join(sorted(scope_names))\n",
    "                    else:\n",
    "                        parsed_data[field] = data['name']\n",
    "                except json.JSONDecodeError:\n",
    "                    parsed_data[field] = \"\"\n",
    "\n",
    "        # Create metadata\n",
    "        metadata = {\n",
    "            \"title\": row.get(\"title\", \"\"),\n",
    "            \"number\": row.get(\"number\", \"\"),\n",
    "            \"type\": row.get(\"type\", \"\"),\n",
    "            \"context\": row.get(\"context\", \"\"),\n",
    "            \"views\": row.get(\"views\", \"\"),\n",
    "            \"role\": parsed_data.get('role', \"\"),\n",
    "            \"role_type\": parsed_data.get('role_type', \"\"),\n",
    "            \"scopes\": parsed_data.get('scopes', \"\"),\n",
    "            \"classifications\": parsed_data.get('classifications', \"\"),\n",
    "            \"organizations\": parsed_data.get('organizations', \"\"),\n",
    "            \"created_at\": row.get(\"created_at\", \"\"),\n",
    "            \"updated_at\": row.get(\"updated_at\", \"\"),\n",
    "            \"row_index\": idx,\n",
    "        }\n",
    "\n",
    "        # Build content sections\n",
    "        content_sections = [\n",
    "            f\"Job Profile Title: {metadata['title']}\",\n",
    "            f\"Classifications: {metadata['classifications']}\",\n",
    "            f\"Organizations: {metadata['organizations']}\"\n",
    "        ]\n",
    "\n",
    "        # Array fields to process\n",
    "        array_fields = {\n",
    "            \"behavioural_competencies\": \"Behavioural Competencies\",\n",
    "            \"education\": \"Education\",\n",
    "            \"job_experience\": \"Job Experience\",\n",
    "            \"professional_registration_requirements\": \"Professional Registration Requirements\",\n",
    "            \"preferences\": \"Preferences\",\n",
    "            \"knowledge_skills_abilities\": \"Knowledge, Skills, and Abilities\",\n",
    "            \"willingness_statements\": \"Willingness Statements\",\n",
    "            \"security_screenings\": \"Security Screenings\",\n",
    "            \"accountabilities\": \"Accountabilities\",\n",
    "        }\n",
    "\n",
    "        # Process each section\n",
    "        for field, section_title in array_fields.items():\n",
    "            if pd.notna(row.get(field)):\n",
    "                try:\n",
    "                    items = json.loads(row[field])\n",
    "                    if(len(items)==0):\n",
    "                        continue\n",
    "                    content_sections.append(f'\\n{row.get(\"title\", \"\")} {section_title}:')\n",
    "                    \n",
    "                    if field == \"behavioural_competencies\":\n",
    "                        \n",
    "                        section_items = [f\"• {item['name']}: {item['description']}\" for item in items]\n",
    "                    else:\n",
    "                        section_items = [f\"• {item['text']}\" for item in items]\n",
    "                    \n",
    "                    content_sections.extend(section_items)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "\n",
    "        # Create one document with all content\n",
    "        doc = Document(\n",
    "            page_content=\"\\n\".join(content_sections),\n",
    "            metadata=metadata\n",
    "        )\n",
    "        documents.append(doc)\n",
    "\n",
    "    return documents\n",
    "documents=process_job_profiles(csvPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_documents_to_console(documents, limit=10):\n",
    "    \"\"\"Prints a preview of the generated documents to the console.\"\"\"\n",
    "    print(f\"Total documents generated: {len(documents)}\\n\")\n",
    "    for i, doc in enumerate(documents[:limit]):  # Limit the number of documents printed\n",
    "        print(f\"Document {i + 1}:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"Content:\\n{doc.page_content}\\n\")\n",
    "        print(f\"Metadata:\\n{json.dumps(doc.metadata, indent=4)}\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "# Dump documents to console\n",
    "dump_documents_to_console(documents, limit=100)  # Adjust limit as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05ad324607104e81be922d615f7da789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a387d6c38914a7dbe89506d207331ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/394 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e794129b85949c59d707b83cd89f90a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a033440edcdd4dca91c6990a2adc846c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b54d60b66ef43cd9377d69e7f17d0fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents adding: 2097\n",
      "creating vector store..\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1abff8306eac4c3abc67997090cd0202",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68bea0a48e054376adf1596931d00eeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/68.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb44fee7c8ee45b4a7a7049f43684af3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "283c6221bacb46c6bc46c50e32acb8cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/66.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5697382c0684026989b9cbfed785630",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling%2Fconfig.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1, size: 100\n",
      "Current document count: 100\n",
      "Processing batch 2, size: 100\n",
      "Current document count: 200\n",
      "Processing batch 3, size: 100\n",
      "Current document count: 300\n",
      "Processing batch 4, size: 100\n",
      "Current document count: 400\n",
      "Processing batch 5, size: 100\n",
      "Current document count: 500\n",
      "Processing batch 6, size: 100\n",
      "Current document count: 600\n",
      "Processing batch 7, size: 100\n",
      "Current document count: 700\n",
      "Processing batch 8, size: 100\n",
      "Current document count: 800\n",
      "Processing batch 9, size: 100\n",
      "Current document count: 900\n",
      "Processing batch 10, size: 100\n",
      "Current document count: 1000\n",
      "Processing batch 11, size: 100\n",
      "Current document count: 1100\n",
      "Processing batch 12, size: 100\n",
      "Current document count: 1200\n",
      "Processing batch 13, size: 100\n",
      "Current document count: 1300\n",
      "Processing batch 14, size: 100\n",
      "Current document count: 1400\n",
      "Processing batch 15, size: 100\n",
      "Current document count: 1500\n",
      "Processing batch 16, size: 100\n",
      "Current document count: 1600\n",
      "Processing batch 17, size: 100\n",
      "Current document count: 1700\n",
      "Processing batch 18, size: 100\n",
      "Current document count: 1800\n",
      "Processing batch 19, size: 100\n",
      "Current document count: 1900\n",
      "Processing batch 20, size: 100\n",
      "Current document count: 2000\n",
      "Processing batch 21, size: 97\n",
      "Current document count: 2097\n",
      "Collection count: 2097\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import CSVLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import chromadb\n",
    "\n",
    "client = chromadb.PersistentClient(\"../job_profiles_db2\")\n",
    "collection = client.get_or_create_collection(\"job_profiles\",metadata={\"hnsw:batch_size\":10000})\n",
    "\n",
    "def create_vectorstore_with_batching(documents, batch_size=100):  # Reduced batch size\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"thenlper/gte-small\")\n",
    "    \n",
    "    vectorstore = Chroma(\n",
    "        # persist_directory=\"job_profiles_db\",\n",
    "        client=client,\n",
    "        embedding_function=embeddings,\n",
    "        collection_name=\"job_profiles\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        for i in range(0, len(documents), batch_size):\n",
    "            batch = documents[i:i + batch_size]\n",
    "            print(f\"Processing batch {i//batch_size + 1}, size: {len(batch)}\")\n",
    "            \n",
    "            vectorstore.add_documents(documents=batch)\n",
    "            \n",
    "            # Add verification step\n",
    "            current_count = vectorstore._collection.count()\n",
    "            print(f\"Current document count: {current_count}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error during processing: {str(e)}\")\n",
    "        \n",
    "    return vectorstore\n",
    "\n",
    "# loader = CSVLoader(file_path=\"../data/job profiles/2025-02-07_profiles.csv\", content_columns=[\"title\", \"overview\"], encoding=\"utf-8-sig\")\n",
    "# documents = loader.load()\n",
    "\n",
    "\n",
    "# text_splitter = RecursiveCharacterTextSplitter(\n",
    "#     chunk_size=1000,\n",
    "#     chunk_overlap=200,\n",
    "#     separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "# )\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    AutoTokenizer.from_pretrained(\"thenlper/gte-small\"),\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=50,\n",
    "    add_start_index=True,\n",
    "    strip_whitespace=True\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "# chunks = documents\n",
    "print(f\"Documents adding: {len(chunks)}\")\n",
    "print('creating vector store..')\n",
    "vectorstore = create_vectorstore_with_batching(chunks)\n",
    "\n",
    "print(f\"Collection count: {vectorstore._collection.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
