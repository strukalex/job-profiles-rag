{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Implementation of Graph RAG for Job Profile Analysis Using Local Ollama Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates a complete Graph RAG implementation for analyzing job profile documents using local Ollama models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -qU langchain langchain_community langchain-experimental neo4j pyvis ollama python-dotenv\n",
    "# !ollama pull llama3.1  # 8B parameter model recommended\n",
    "# !ollama pull nomic-embed-text  # Embedding model\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('..'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Document Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TESTING=False           # Set to True to run with toy data\n",
    "RUN_IN_BATCH=True       # If False, will try to run all chunks at once to get entity connections, otherwise will save after each one\n",
    "USE_OLLAMA=True         # Whether to use local Ollama or Azure API\n",
    "LIMIT_CHUNKS=None       # Set to a number to limit the number of chunk to be processed\n",
    "CHUNK_SIZE=1000          \n",
    "csvPath=\"../data/job profiles/2025-02-07_profiles.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = MistralTokenizer.from_model(\"mistral-small\", strict=True)\n",
    "# text = \"Your text here\"\n",
    "# tokens = tokenizer.encode_chat_completion(text)\n",
    "# token_count = len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "import pandas as pd\n",
    "from mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n",
    "from mistral_common.protocol.instruct.request import ChatCompletionRequest\n",
    "from mistral_common.protocol.instruct.messages import UserMessage\n",
    "\n",
    "class JobProfile(BaseModel):\n",
    "    title: str = Field(description=\"Official job title\")\n",
    "    classifications: List[str] = Field(description=\"Classification codes\")\n",
    "    organizations: List[str] \n",
    "    behavioural_competencies: List[str]\n",
    "    education: List[str] = Field(description=\"Education requirements\")\n",
    "    job_experience: List[str]\n",
    "    knowledge_skills_abilities: List[str]\n",
    "    security_screenings: List[str]\n",
    "    accountabilities: List[str]\n",
    "    role_type: Optional[str] = Field(description=\"Role category\")\n",
    "    scopes: Optional[List[str]] = Field(description=\"Areas of responsibility\")\n",
    "    professional_registration: Optional[List[str]]\n",
    "\n",
    "if not TESTING:\n",
    "    from notebooks.utils import get_job_profile_documents\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    from transformers import AutoTokenizer\n",
    "\n",
    "    documents=get_job_profile_documents(csvPath)\n",
    "    tokenizer = MistralTokenizer.from_model(\"mistral-small\", strict=True)\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        chunk_overlap=200,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \"â€¢\", \" \", \"\"],\n",
    "        length_function=lambda text: len(tokenizer.encode_chat_completion(\n",
    "            ChatCompletionRequest(\n",
    "                messages=[\n",
    "                    UserMessage(content=text)\n",
    "                ],\n",
    "                model=\"mistral-small-latest\"\n",
    "            )\n",
    "        ).tokens)\n",
    "    )\n",
    "    \n",
    "    chunks = splitter.split_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LIMIT_CHUNKS is not None:\n",
    "    chunks=chunks[0:LIMIT_CHUNKS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'version', 'title', 'number', 'overview', 'program_overview',\n",
       "       'state', 'type', 'behavioural_competencies', 'accountabilities',\n",
       "       'education', 'job_experience', 'professional_registration_requirements',\n",
       "       'preferences', 'knowledge_skills_abilities', 'willingness_statements',\n",
       "       'optional_requirements', 'security_screenings', 'all_reports_to',\n",
       "       'context', 'is_archived', 'valid_from', 'valid_to', 'views', 'role',\n",
       "       'role_type', 'created_at', 'updated_at', 'published_at',\n",
       "       'classifications', 'organizations', 'scopes', 'job_families', 'streams',\n",
       "       'reports_to'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csvPath=\"../data/job profiles/2025-02-07_profiles.csv\"\n",
    "df=pd.read_csv(csvPath)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TESTING:\n",
    "    from langchain_community.document_loaders import TextLoader\n",
    "    from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "    # Custom job profile document format\n",
    "    job_profiles = \"\"\"\n",
    "    [Job Profile: Data Scientist]\n",
    "    Accountabilities:\n",
    "    - Develop ML models for customer segmentation\n",
    "    - Collaborate with engineering teams on deployment\n",
    "\n",
    "    Knowledge:\n",
    "    - Advanced statistics\n",
    "    - Python programming\n",
    "\n",
    "    Skills:\n",
    "    - TensorFlow/PyTorch\n",
    "    - SQL optimization\n",
    "\n",
    "    [Job Profile: Cloud Architect]\n",
    "    Accountabilities:\n",
    "    - Design AWS infrastructure\n",
    "    - Implement security protocols\n",
    "\n",
    "    Knowledge:\n",
    "    - Networking fundamentals\n",
    "    - IaaS/PaaS/SaaS models\n",
    "\n",
    "    Skills:\n",
    "    - Terraform infrastructure as code\n",
    "    - Cost optimization techniques\n",
    "    \"\"\"\n",
    "\n",
    "    with open(\"job_profiles.txt\", \"w\") as f:\n",
    "        f.write(job_profiles)\n",
    "\n",
    "    loader = TextLoader(\"job_profiles.txt\")\n",
    "    docs = loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        separators=[\"\\n\\n[Job Profile:\", \"\\n\\nAccountabilities:\", \"\\n\\nKnowledge:\"]\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Graph Construction with LLM Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To View available models in ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_OLLAMA:\n",
    "    import ollama\n",
    "    from datetime import datetime\n",
    "\n",
    "    # Get models\n",
    "    models = ollama.Client().list()['models']\n",
    "\n",
    "    print(\"Available Models:\")\n",
    "    print(\"-\" * 80)\n",
    "    for model in models:\n",
    "        # Extract model name from model field\n",
    "        model_name = model['model']\n",
    "        \n",
    "        # Format size in GB\n",
    "        size_gb = model['size'] / 1_000_000_000\n",
    "        \n",
    "        # Format datetime\n",
    "        modified = model['modified_at'].strftime(\"%Y-%m-%d %H:%M:%S %Z\")\n",
    "        \n",
    "        # Print main model info\n",
    "        print(f\"Model: {model_name}\")\n",
    "        print(f\"Modified: {modified}\")\n",
    "        print(f\"Size: {size_gb:.2f} GB\")\n",
    "        \n",
    "        # Print model details if available\n",
    "        if 'details' in model:\n",
    "            details = model['details']\n",
    "            print(\"Details:\")\n",
    "            print(f\"  Format: {details.format}\")\n",
    "            print(f\"  Family: {details.family}\")\n",
    "            if hasattr(details, 'parameter_size'):\n",
    "                print(f\"  Parameter Size: {details.parameter_size}\")\n",
    "            if hasattr(details, 'quantization_level'):\n",
    "                print(f\"  Quantization: {details.quantization_level}\")\n",
    "        \n",
    "        print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the llm and graph_transformer (communicates with the llm and generates queries for entity relationship extraction out of chunks):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated code\n",
    "from langchain_ollama import OllamaLLM  # New import path\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from langchain_azure_ai.chat_models import AzureAIChatCompletionsModel\n",
    "\n",
    "# Initialize with updated model naming format\n",
    "if USE_OLLAMA:\n",
    "    llm = OllamaLLM(model=\"hf.co/bartowski/cognitivecomputations_Dolphin3.0-Mistral-24B-GGUF:latest\", temperature=0)\n",
    "else:\n",
    "    llm = AzureAIChatCompletionsModel(\n",
    "                endpoint=os.getenv('AZURE_ENDPOINT'),\n",
    "                credential=os.getenv('AZURE_API_KEY'),\n",
    "                model_name=\"Mistral-small\",\n",
    "                api_version=\"2024-05-01-preview\",\n",
    "                model_kwargs={\"max_tokens\": 4000},\n",
    "                \n",
    "                temperature=0.5,\n",
    "                top_p=0.4\n",
    "            )\n",
    "graph_transformer = LLMGraphTransformer(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_debug\n",
    "# import logging\n",
    "\n",
    "# Enable verbose logging for all components\n",
    "set_debug(True)\n",
    "\n",
    "# import logging\n",
    "# logging.basicConfig(level=logging.INFO) # DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check LLM connectivity\n",
    "# from langchain.callbacks.base import BaseCallbackHandler\n",
    "\n",
    "# class CleanOutputHandler(BaseCallbackHandler):\n",
    "#     def on_llm_start(self, serialized, prompts, **kwargs):\n",
    "#         # print(\"\\n=== LLM Start ===\")\n",
    "#         print(f\"====== LLM INPUT ====== \\n\\n: {prompts[0]}\")\n",
    "#         # print(f\"Serialized data: {serialized}\")\n",
    "#         # print(f\"Additional kwargs: {kwargs}\")\n",
    "        \n",
    "#     def on_llm_end(self, response, **kwargs):\n",
    "#         # print(\"\\n=== LLM End ===\")\n",
    "#         if hasattr(response, 'generations'):\n",
    "#             for i, generation_list in enumerate(response.generations):\n",
    "#                 for j, generation in enumerate(generation_list):\n",
    "#                     print(f\"====== LLM OUTPUT ====== \\n\\n {i}.{j}: {generation.text}\")\n",
    "#         # print(f\"Additional kwargs: {kwargs}\")\n",
    "\n",
    "# test_response = llm.invoke(\"Say 'hello' if you can read this.\", config={\"callbacks\": [CleanOutputHandler()]})\n",
    "# print(\"Connection successful. Response:\", test_response)\n",
    "\n",
    "# Non-debug configuration\n",
    "# graph_transformer = LLMGraphTransformer(llm=llm)\n",
    "\n",
    "# # Transform documents into graph nodes/relationships\n",
    "# graph_documents = graph_transformer.convert_to_graph_documents(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "class DebugLLMGraphTransformer(LLMGraphTransformer):\n",
    "    def process_response(self, document: Document, config=None):\n",
    "        print(f\"Processing document: {document.page_content}\")\n",
    "        \n",
    "        result = super().process_response(document, config)\n",
    "        print(f\"Transformed result: {result}\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "        # return result\n",
    "\n",
    "# Initialize with debug transformer\n",
    "if TESTING:\n",
    "    debug_transformer = DebugLLMGraphTransformer(\n",
    "        llm=llm,\n",
    "        # strict_mode=False,  # Disable strict filtering during debugging\n",
    "    )\n",
    "else:\n",
    "    if not USE_OLLAMA:\n",
    "        debug_transformer = DebugLLMGraphTransformer(\n",
    "        llm=llm,\n",
    "        node_properties={\n",
    "            \"Jobprofile\": [\"id\"],  # Matches database label and property\n",
    "            \"Classification\": [\"id\"],\n",
    "            \"Organization\": [\"id\"],\n",
    "            \"Behaviouralcompetency\": [\"id\"],  # Matches exact label spelling\n",
    "            \"Document\": [\"title\"],  # Title exists on Document nodes\n",
    "            \"__Entity__\": [\"id\"],  # From constraint in visualization\n",
    "            \"Education\": [\"requirement\"],\n",
    "            \"Experience\": [\"requirement\"],\n",
    "            \"SecurityScreening\": [\"requirement\"],\n",
    "            \"Accountability\": [\"description\"]\n",
    "        },\n",
    "        relationship_properties={\n",
    "            # All observed relationships\n",
    "            \"HAS_CLASSIFICATION\": {},\n",
    "            \"BELONGS_TO_ORGANIZATION\": {},\n",
    "            \"REQUIRES_COMPETENCY\": {},\n",
    "            \"MENTIONS\": {},  # Critical missing relationship\n",
    "            \"HAS_EDUCATION_REQUIREMENT\": {},\n",
    "            \"REQUIRES_EXPERIENCE\": {},\n",
    "            \"REQUIRES_SCREENING\": {},\n",
    "            \"HAS_ACCOUNTABILITY\": {}\n",
    "        }\n",
    "        )\n",
    "    else:\n",
    "        # must remove restrictions for local ollama\n",
    "        debug_transformer = DebugLLMGraphTransformer(\n",
    "            llm=llm,\n",
    "            # strict_mode=True  # Keep strict filtering if needed\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Neo4j Graph Database Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_neo4j import Neo4jGraph, GraphCypherQAChain\n",
    "import os\n",
    "\n",
    "os.environ[\"NEO4J_URI\"] = \"bolt://localhost:7687\"\n",
    "os.environ[\"NEO4J_USERNAME\"] = \"neo4j\"\n",
    "os.environ[\"NEO4J_PASSWORD\"] = \"your_password\"\n",
    "\n",
    "graph = Neo4jGraph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test out neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': '69116869CB35145331EE2540ACCB41CE6769FA0EF563941B7EF46E8944A02587', 'name': 'neo4j', 'creationDate': '2025-02-23T22:56:33.286Z'}]\n"
     ]
    }
   ],
   "source": [
    "print(graph.query(\"CALL db.info()\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processed up to:\n",
    "# Inserted 1 graph elements from batch 188\n",
    "# Processing document batch 189/1130\n",
    "# Clear existing data\n",
    "graph.query(\"MATCH (n) DETACH DELETE n\")\n",
    "\n",
    "if not RUN_IN_BATCH:\n",
    "    graph_documents = debug_transformer.convert_to_graph_documents(chunks)\n",
    "    print(graph_documents)\n",
    "\n",
    "    # Load your GraphDocument data\n",
    "    graph.add_graph_documents(\n",
    "        graph_documents,\n",
    "        baseEntityLabel=True,    # Adds __Entity__ label for better indexing\n",
    "        include_source=True      # Maintains document source relationships\n",
    "    )\n",
    "else:\n",
    "    # Process documents in batches\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"Processing document batch {i+1}/{len(chunks)}\")\n",
    "        \n",
    "        # Convert single chunk to graph document\n",
    "        graph_doc = debug_transformer.convert_to_graph_documents([chunk])  # Wrap in list\n",
    "        \n",
    "        # Add to database immediately\n",
    "        graph.add_graph_documents(\n",
    "            graph_doc,\n",
    "            baseEntityLabel=True,\n",
    "            include_source=True\n",
    "        )\n",
    "        \n",
    "        # Optional: Add progress tracking\n",
    "        print(f\"Inserted {len(graph_doc)} graph elements from batch {i+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create nodes and relationships\n",
    "# graph.query(\n",
    "#     \"\"\"\n",
    "# MERGE (m:Movie {name:\"Top Gun\", runtime: 120})\n",
    "# WITH m\n",
    "# UNWIND [\"Tom Cruise\", \"Val Kilmer\", \"Anthony Edwards\", \"Meg Ryan\"] AS actor\n",
    "# MERGE (a:Actor {name:actor})\n",
    "# MERGE (a)-[:ACTED_IN]->(m)\n",
    "# \"\"\"\n",
    "# )\n",
    "# graph.refresh_schema()\n",
    "# print(graph.schema)\n",
    "# chain = GraphCypherQAChain.from_llm(\n",
    "#     llm=llm, graph=graph, verbose=True, allow_dangerous_requests=True\n",
    "# )\n",
    "# chain.invoke({\"query\": \"Who played in Top Gun?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.query(\"CALL db.schema.visualization()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.refresh_schema()\n",
    "print(graph.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check relationship count in database\n",
    "result = graph.query(\"\"\"\n",
    "    MATCH ()-[r]->() \n",
    "    RETURN count(r) AS relationship_count,\n",
    "           collect(distinct type(r)) AS relationship_types\n",
    "\"\"\")\n",
    "print(f\"Relationships Found: {result[0]['relationship_count']}\")\n",
    "print(f\"Relationship Types: {result[0]['relationship_types']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all Jobprofiles and their organizations\n",
    "result = graph.query(\"MATCH (jp:Jobprofile)-[:BELONGS_TO_ORGANIZATION]->(org:Organization) RETURN jp.id, org.id\")\n",
    "print(result)\n",
    "print('========')\n",
    "# Get all Documents mentioning Licensing Clerk\n",
    "print(graph.query(\"MATCH (d:Document {title: 'Licensing Clerk'})-[:MENTIONS]->(jp:Jobprofile) RETURN d.title, jp.id\"))\n",
    "print('====== ai not workign: ')\n",
    "print(graph.query(\"MATCH (jp:Jobprofile {title: 'Licensing Clerk'})-[:BELONGS_TO_ORGANIZATION]->(o:Organization) RETURN o.id\")) # AI generated - not wokring - confused title with id\n",
    "print('====== same prompt but claude: ')\n",
    "print(graph.query(\"MATCH (j:Jobprofile)-[:BELONGS_TO_ORGANIZATION]->(o:Organization) WHERE j.id = 'Licensing Clerk' RETURN o.id\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = GraphCypherQAChain.from_llm(\n",
    "    llm=llm, graph=graph, verbose=True, allow_dangerous_requests=True,\n",
    "    # exclude_types=['Document']\n",
    "    # validate_cypher=True,  # New critical parameter\n",
    "    # schema_constraints={\n",
    "    #     \"Jobprofile\": {\"identifier\": \"id\"},  # Force 'id' usage\n",
    "    #     \"Document\": {\"identifier\": \"title\"}\n",
    "    # }\n",
    ")\n",
    "chain.invoke({\"query\": \"What organizations does the 'Licensing Clerk' profile belong to? Ensure title is treated as 'id' instead of 'title'\"})\n",
    "# use backticks for labels containing spaces: e.g. MATCH (jt:`Job Title` ========== \\n "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Graph Visualization with Pyvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %% [code]\n",
    "# from pyvis.network import Network\n",
    "\n",
    "# # Initialize network with configuration\n",
    "# net = Network(\n",
    "#     notebook=True, \n",
    "#     cdn_resources=\"in_line\", \n",
    "#     height=\"750px\"\n",
    "# )\n",
    "\n",
    "# # Add nodes with metadata\n",
    "# nodes = graph.query(\"\"\"\n",
    "#     MATCH (n) \n",
    "#     RETURN n.id as id, \n",
    "#            n.name as label, \n",
    "#            n.type as group\n",
    "# \"\"\")\n",
    "\n",
    "# # Process each node and add to network\n",
    "# for node in nodes:\n",
    "#     net.add_node(\n",
    "#         node[\"id\"],\n",
    "#         label=node[\"label\"],\n",
    "#         group=node[\"group\"],\n",
    "#         title=f\"Type: {node['group']}\"\n",
    "#     )\n",
    "\n",
    "# # Add relationships with labels\n",
    "# relationships = graph.query(\"\"\"\n",
    "#     MATCH (s)-[r]->(t) \n",
    "#     RETURN s.id as source, \n",
    "#            t.id as target, \n",
    "#            type(r) as label\n",
    "# \"\"\")\n",
    "\n",
    "# # Process each relationship and add to network\n",
    "# for rel in relationships:\n",
    "#     net.add_edge(\n",
    "#         rel[\"source\"], \n",
    "#         rel[\"target\"],\n",
    "#         label=rel[\"label\"],\n",
    "#         arrowStrikethrough=False\n",
    "#     )\n",
    "\n",
    "# # Generate and save interactive visualization\n",
    "# net.show(\"job_network.html\")\n",
    "\n",
    "# NEW\n",
    "\n",
    "# %% [code]\n",
    "from pyvis.network import Network\n",
    "\n",
    "# Initialize network with optimized configuration\n",
    "net = Network(\n",
    "    notebook=True, \n",
    "    cdn_resources=\"in_line\",\n",
    "    # height=\"750px\",\n",
    "    # width=\"100%\",\n",
    "    # layout={\n",
    "    #     # \"hierarchical\": {\"enabled\": True},\n",
    "    #     # \"levelSeparation\": 150,    # Vertical spacing between levels [5]\n",
    "    #     \"nodeSpacing\": 200,        # Minimum horizontal spacing [5]\n",
    "    #     \"treeSpacing\": 300         # Spacing between disconnected components [5]\n",
    "    # }\n",
    ")\n",
    "\n",
    "\n",
    "# Configure physics system for optimal node distribution [1][4][7]\n",
    "# net.set_options(\"\"\"\n",
    "# {\n",
    "#     \"physics\": {\n",
    "#         \"enabled\": true,\n",
    "#         \"solver\": \"forceAtlas2Based\",\n",
    "#         \"forceAtlas2Based\": {\n",
    "#             \"gravitationalConstant\": -150,\n",
    "#             \"centralGravity\": 0.01,\n",
    "#             \"springLength\": 250,\n",
    "#             \"springConstant\": 0.005,\n",
    "#             \"damping\": 0.4,\n",
    "#             \"avoidOverlap\": 1.0\n",
    "#         },\n",
    "#         \"maxVelocity\": 75,\n",
    "#         \"minVelocity\": 2,\n",
    "#         \"timestep\": 0.5\n",
    "#     }\n",
    "# }\n",
    "# \"\"\")\n",
    "\n",
    "# Add nodes with extended metadata for visual clarity [9]\n",
    "nodes = graph.query(\"\"\"\n",
    "    MATCH (n) \n",
    "    RETURN n.id as id, \n",
    "           n.name as label, \n",
    "           n.type as group\n",
    "\"\"\")\n",
    "\n",
    "for node in nodes:\n",
    "    net.add_node(\n",
    "        node[\"id\"],\n",
    "        label=node[\"label\"],\n",
    "        group=node[\"group\"],\n",
    "        title=f\"\"\"\n",
    "            Type: {node['group']}\n",
    "            Connections: {node.get('degree', 0)}\n",
    "        \"\"\",\n",
    "        value=node.get(\"value\", 10),  # Default size if missing [9]\n",
    "        borderWidth=2,                # Clear node boundaries [9]\n",
    "        shape=\"dot\",                  # Consistent node shape\n",
    "        font={\"size\": 18}             # Improved label readability\n",
    "    )\n",
    "\n",
    "# Add relationships with enhanced visual properties [1][10]\n",
    "relationships = graph.query(\"\"\"\n",
    "    MATCH (s)-[r]->(t) \n",
    "    RETURN s.id as source, \n",
    "           t.id as target, \n",
    "           type(r) as label\n",
    "\"\"\")\n",
    "\n",
    "for rel in relationships:\n",
    "    net.add_edge(\n",
    "        rel[\"source\"], \n",
    "        rel[\"target\"],\n",
    "        label=rel[\"label\"],\n",
    "        value=rel.get(\"value\", 1),    # Default edge weight [10]\n",
    "        smooth={\"type\": \"dynamic\"},   # Curved edge rendering [12]\n",
    "        arrowStrikethrough=False,\n",
    "        # color={\n",
    "        #     \"color\": \"#95a5a6\",       # Base edge color\n",
    "        #     \"highlight\": \"#3498db\"    # Highlight color on hover\n",
    "        # },\n",
    "        # width=2,                      # Visual weight multiplier [10]\n",
    "        physics=True                   # Enable edge spring behavior [9]\n",
    "    )\n",
    "\n",
    "# Final layout optimization steps [2][6]\n",
    "# net.toggle_physics(True)             # Enable for initial stabilization\n",
    "# net.show_buttons(filter_=['physics']) # Allow parameter adjustments [6]\n",
    "\n",
    "net.set_options(\"\"\"\n",
    "{\n",
    "    \"physics\": {\n",
    "        \"enabled\": true,\n",
    "        \"solver\": \"repulsion\",\n",
    "        \"repulsion\": {\n",
    "            \"nodeDistance\": 300,\n",
    "            \"springLength\": 200\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "# Generate and save visualization with preservation options\n",
    "net.write_html(\n",
    "    \"job_network.html\",\n",
    "    local=False,\n",
    "    notebook=False,\n",
    "    # override=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now add vector embedding index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to existing Neo4jGraph initialization\n",
    "from langchain_community.vectorstores import Neo4jVector\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"thenlper/gte-small\")\n",
    "\n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "# his method pulls relevant text information from the database, and calculates and stores the text embeddings back to the database.\n",
    "vector_store = Neo4jVector.from_existing_graph(\n",
    "    embedding=embeddings,\n",
    "    url=os.environ[\"NEO4J_URI\"],\n",
    "    username=os.environ[\"NEO4J_USERNAME\"],\n",
    "    password=os.environ[\"NEO4J_PASSWORD\"],\n",
    "    index_name=\"document_embeddings\",\n",
    "    node_label=\"Jobprofile\",\n",
    "    text_node_properties=[\"text\",\"title\",\"id\"],\n",
    "    embedding_node_property=\"embedding\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_embedding = embeddings.embed_query(\"test\")\n",
    "print(len(sample_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimized vector index\n",
    "# todo: is this needed?\n",
    "graph.query(\"\"\"\n",
    "CREATE VECTOR INDEX document_embeddings IF NOT EXISTS\n",
    "FOR (n:Document) ON (n.embedding)\n",
    "OPTIONS {\n",
    "  indexConfig: {\n",
    "    `vector.dimensions`: 384,\n",
    "    `vector.similarity_function`: 'cosine'\n",
    "  }\n",
    "}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vector_store.similarity_search(\"Agriculture\", k=3)\n",
    "print([doc.page_content for doc in results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
