{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Implementation of Graph RAG for Job Profile Analysis Using Local Ollama Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates a complete Graph RAG implementation for analyzing job profile documents using local Ollama models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -qU langchain langchain_community langchain-experimental neo4j pyvis ollama python-dotenv\n",
    "# !ollama pull llama3.1  # 8B parameter model recommended\n",
    "# !ollama pull nomic-embed-text  # Embedding model\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('..'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Document Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TESTING=False           # Set to True to run with toy data\n",
    "RUN_IN_BATCH=True       # If False, will try to run all chunks at once to get entity connections, otherwise will save after each one\n",
    "USE_OLLAMA=False         # Whether to use local Ollama or Azure API\n",
    "LIMIT_CHUNKS=1       # Set to a number to limit the number of chunk to be processed\n",
    "CHUNK_SIZE=1000          \n",
    "csvPath=\"../data/job profiles/2025-02-07_profiles.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = MistralTokenizer.from_model(\"mistral-small\", strict=True)\n",
    "# text = \"Your text here\"\n",
    "# tokens = tokenizer.encode_chat_completion(text)\n",
    "# token_count = len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "import pandas as pd\n",
    "from mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n",
    "from mistral_common.protocol.instruct.request import ChatCompletionRequest\n",
    "from mistral_common.protocol.instruct.messages import UserMessage\n",
    "\n",
    "class JobProfile(BaseModel):\n",
    "    title: str = Field(description=\"Official job title\")\n",
    "    classifications: List[str] = Field(description=\"Classification codes\")\n",
    "    organizations: List[str] \n",
    "    behavioural_competencies: List[str]\n",
    "    education: List[str] = Field(description=\"Education requirements\")\n",
    "    job_experience: List[str]\n",
    "    knowledge_skills_abilities: List[str]\n",
    "    security_screenings: List[str]\n",
    "    accountabilities: List[str]\n",
    "    role_type: Optional[str] = Field(description=\"Role category\")\n",
    "    scopes: Optional[List[str]] = Field(description=\"Areas of responsibility\")\n",
    "    professional_registration: Optional[List[str]]\n",
    "\n",
    "if not TESTING:\n",
    "    from notebooks.utils import get_job_profile_documents\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    from transformers import AutoTokenizer\n",
    "\n",
    "    documents=get_job_profile_documents(csvPath, include_org_class_sections=False)\n",
    "    tokenizer = MistralTokenizer.from_model(\"mistral-small\", strict=True)\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        chunk_overlap=200,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \"•\", \" \", \"\"],\n",
    "        length_function=lambda text: len(tokenizer.encode_chat_completion(\n",
    "            ChatCompletionRequest(\n",
    "                messages=[\n",
    "                    UserMessage(content=text)\n",
    "                ],\n",
    "                model=\"mistral-small-latest\"\n",
    "            )\n",
    "        ).tokens)\n",
    "    )\n",
    "    \n",
    "    chunks = splitter.split_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LIMIT_CHUNKS is not None:\n",
    "    chunks=chunks[0:LIMIT_CHUNKS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvPath=\"../data/job profiles/2025-02-07_profiles.csv\"\n",
    "df=pd.read_csv(csvPath)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TESTING:\n",
    "    from langchain_community.document_loaders import TextLoader\n",
    "    from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "    # Custom job profile document format\n",
    "    job_profiles = \"\"\"\n",
    "    [Job Profile: Data Scientist]\n",
    "    Accountabilities:\n",
    "    - Develop ML models for customer segmentation\n",
    "    - Collaborate with engineering teams on deployment\n",
    "\n",
    "    Knowledge:\n",
    "    - Advanced statistics\n",
    "    - Python programming\n",
    "\n",
    "    Skills:\n",
    "    - TensorFlow/PyTorch\n",
    "    - SQL optimization\n",
    "\n",
    "    [Job Profile: Cloud Architect]\n",
    "    Accountabilities:\n",
    "    - Design AWS infrastructure\n",
    "    - Implement security protocols\n",
    "\n",
    "    Knowledge:\n",
    "    - Networking fundamentals\n",
    "    - IaaS/PaaS/SaaS models\n",
    "\n",
    "    Skills:\n",
    "    - Terraform infrastructure as code\n",
    "    - Cost optimization techniques\n",
    "    \"\"\"\n",
    "\n",
    "    with open(\"job_profiles.txt\", \"w\") as f:\n",
    "        f.write(job_profiles)\n",
    "\n",
    "    loader = TextLoader(\"job_profiles.txt\")\n",
    "    docs = loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        separators=[\"\\n\\n[Job Profile:\", \"\\n\\nAccountabilities:\", \"\\n\\nKnowledge:\"]\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Graph Construction with LLM Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To View available models in ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_OLLAMA:\n",
    "    import ollama\n",
    "    from datetime import datetime\n",
    "\n",
    "    # Get models\n",
    "    models = ollama.Client().list()['models']\n",
    "\n",
    "    print(\"Available Models:\")\n",
    "    print(\"-\" * 80)\n",
    "    for model in models:\n",
    "        # Extract model name from model field\n",
    "        model_name = model['model']\n",
    "        \n",
    "        # Format size in GB\n",
    "        size_gb = model['size'] / 1_000_000_000\n",
    "        \n",
    "        # Format datetime\n",
    "        modified = model['modified_at'].strftime(\"%Y-%m-%d %H:%M:%S %Z\")\n",
    "        \n",
    "        # Print main model info\n",
    "        print(f\"Model: {model_name}\")\n",
    "        print(f\"Modified: {modified}\")\n",
    "        print(f\"Size: {size_gb:.2f} GB\")\n",
    "        \n",
    "        # Print model details if available\n",
    "        if 'details' in model:\n",
    "            details = model['details']\n",
    "            print(\"Details:\")\n",
    "            print(f\"  Format: {details.format}\")\n",
    "            print(f\"  Family: {details.family}\")\n",
    "            if hasattr(details, 'parameter_size'):\n",
    "                print(f\"  Parameter Size: {details.parameter_size}\")\n",
    "            if hasattr(details, 'quantization_level'):\n",
    "                print(f\"  Quantization: {details.quantization_level}\")\n",
    "        \n",
    "        print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the llm and graph_transformer (communicates with the llm and generates queries for entity relationship extraction out of chunks):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated code\n",
    "from langchain_ollama import OllamaLLM  # New import path\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from langchain_azure_ai.chat_models import AzureAIChatCompletionsModel\n",
    "\n",
    "# Initialize with updated model naming format\n",
    "if USE_OLLAMA:\n",
    "    llm = OllamaLLM(model=\"hf.co/bartowski/cognitivecomputations_Dolphin3.0-Mistral-24B-GGUF:latest\", temperature=0)\n",
    "else:\n",
    "    llm = AzureAIChatCompletionsModel(\n",
    "                endpoint=os.getenv('AZURE_ENDPOINT'),\n",
    "                credential=os.getenv('AZURE_API_KEY'),\n",
    "                model_name=\"Mistral-small\",\n",
    "                api_version=\"2024-05-01-preview\",\n",
    "                model_kwargs={\"max_tokens\": 4000},\n",
    "                \n",
    "                temperature=0.5,\n",
    "                top_p=0.4\n",
    "            )\n",
    "graph_transformer = LLMGraphTransformer(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_debug\n",
    "# import logging\n",
    "\n",
    "# Enable verbose logging for all components\n",
    "set_debug(True)\n",
    "\n",
    "# import logging\n",
    "# logging.basicConfig(level=logging.INFO) # DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check LLM connectivity\n",
    "# from langchain.callbacks.base import BaseCallbackHandler\n",
    "\n",
    "# class CleanOutputHandler(BaseCallbackHandler):\n",
    "#     def on_llm_start(self, serialized, prompts, **kwargs):\n",
    "#         # print(\"\\n=== LLM Start ===\")\n",
    "#         print(f\"====== LLM INPUT ====== \\n\\n: {prompts[0]}\")\n",
    "#         # print(f\"Serialized data: {serialized}\")\n",
    "#         # print(f\"Additional kwargs: {kwargs}\")\n",
    "        \n",
    "#     def on_llm_end(self, response, **kwargs):\n",
    "#         # print(\"\\n=== LLM End ===\")\n",
    "#         if hasattr(response, 'generations'):\n",
    "#             for i, generation_list in enumerate(response.generations):\n",
    "#                 for j, generation in enumerate(generation_list):\n",
    "#                     print(f\"====== LLM OUTPUT ====== \\n\\n {i}.{j}: {generation.text}\")\n",
    "#         # print(f\"Additional kwargs: {kwargs}\")\n",
    "\n",
    "# test_response = llm.invoke(\"Say 'hello' if you can read this.\", config={\"callbacks\": [CleanOutputHandler()]})\n",
    "# print(\"Connection successful. Response:\", test_response)\n",
    "\n",
    "# Non-debug configuration\n",
    "# graph_transformer = LLMGraphTransformer(llm=llm)\n",
    "\n",
    "# # Transform documents into graph nodes/relationships\n",
    "# graph_documents = graph_transformer.convert_to_graph_documents(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "from langchain_core.documents import Document\n",
    "from langchain.graphs.graph_document import Node, Relationship\n",
    "\n",
    "class DebugLLMGraphTransformer(LLMGraphTransformer):\n",
    "    def process_response(self, document: Document, config=None):\n",
    "        print(f\"Processing document: {document.page_content}\")\n",
    "        \n",
    "        graph_doc = super().process_response(document, config)\n",
    "        print(f\"Transformed result: {graph_doc}\")\n",
    "        \n",
    "        # Extract classifications from metadata\n",
    "        # if 'classifications' in document.metadata:\n",
    "\n",
    "            # Create source document node with consistent ID\n",
    "            # doc_id = document.metadata.get(\"id\") or hashlib.md5(document.page_content.encode()).hexdigest()\n",
    "            # doc_node = Node(id=doc_id,type=\"Document\")\n",
    "            \n",
    "            # Node(\n",
    "            #     id=doc_id,\n",
    "            #     type=\"Document\",\n",
    "            #     properties={\n",
    "            #         \"text\": document.page_content,\n",
    "            #         **document.metadata\n",
    "            #     }\n",
    "            # )\n",
    "            # graph_doc.nodes.append(doc_node)\n",
    "\n",
    "            # classifications = document.metadata['classifications'].split(',')\n",
    "            \n",
    "            # # Create Classification nodes and relationships\n",
    "            # for classification in classifications:\n",
    "            #     classification = classification.strip()\n",
    "            #     if classification:\n",
    "            #         # Create a Classification node\n",
    "            #         class_node = Node(id=classification, type=\"Classification\", properties={\"code\": classification})\n",
    "            #         graph_doc.nodes.append(class_node)\n",
    "                    \n",
    "            #         # Link it to the source Document (not individual entities)\n",
    "            #         # The source will become a Document node in Neo4j\n",
    "            #         graph_doc.relationships.append(\n",
    "            #             Relationship(source=doc_node, target=class_node, type=\"HAS_CLASSIFICATION\")\n",
    "            #         )\n",
    "\n",
    "        return graph_doc\n",
    "    \n",
    "        # return result\n",
    "\n",
    "# Initialize with debug transformer\n",
    "if TESTING:\n",
    "    debug_transformer = DebugLLMGraphTransformer(\n",
    "        llm=llm,\n",
    "        # strict_mode=False,  # Disable strict filtering during debugging\n",
    "    )\n",
    "else:\n",
    "    if not USE_OLLAMA:\n",
    "        debug_transformer = DebugLLMGraphTransformer(\n",
    "            llm=llm,\n",
    "            # strict_mode=True  # Keep strict filtering if needed\n",
    "        )\n",
    "        # debug_transformer = DebugLLMGraphTransformer(\n",
    "        # llm=llm,\n",
    "        # node_properties={\n",
    "        #     \"Jobprofile\": [\"id\"],  # Matches database label and property\n",
    "        #     \"Classification\": [\"id\"],\n",
    "        #     \"Organization\": [\"id\"],\n",
    "        #     \"Behaviouralcompetency\": [\"id\"],  # Matches exact label spelling\n",
    "        #     \"Document\": [\"title\"],  # Title exists on Document nodes\n",
    "        #     \"__Entity__\": [\"id\"],  # From constraint in visualization\n",
    "        #     \"Education\": [\"requirement\"],\n",
    "        #     \"Experience\": [\"requirement\"],\n",
    "        #     \"SecurityScreening\": [\"requirement\"],\n",
    "        #     \"Accountability\": [\"description\"]\n",
    "        # },\n",
    "        # relationship_properties={\n",
    "        #     # All observed relationships\n",
    "        #     \"HAS_CLASSIFICATION\": {},\n",
    "        #     \"BELONGS_TO_ORGANIZATION\": {},\n",
    "        #     \"REQUIRES_COMPETENCY\": {},\n",
    "        #     \"MENTIONS\": {},  # Critical missing relationship\n",
    "        #     \"HAS_EDUCATION_REQUIREMENT\": {},\n",
    "        #     \"REQUIRES_EXPERIENCE\": {},\n",
    "        #     \"REQUIRES_SCREENING\": {},\n",
    "        #     \"HAS_ACCOUNTABILITY\": {}\n",
    "        # }\n",
    "        # )\n",
    "    else:\n",
    "        # must remove restrictions for local ollama - this will process text as unstructured data\n",
    "        # an extract arbitrary entities and relationships\n",
    "        debug_transformer = DebugLLMGraphTransformer(\n",
    "            llm=llm,\n",
    "            # strict_mode=True  # Keep strict filtering if needed\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Neo4j Graph Database Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize db connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_neo4j import Neo4jGraph, GraphCypherQAChain\n",
    "import os\n",
    "\n",
    "os.environ[\"NEO4J_URI\"] = \"bolt://localhost:7687\"\n",
    "os.environ[\"NEO4J_USERNAME\"] = \"neo4j\"\n",
    "os.environ[\"NEO4J_PASSWORD\"] = \"your_password\"\n",
    "\n",
    "graph = Neo4jGraph()\n",
    "print(graph.query(\"CALL db.info()\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing of neo4j db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create nodes and relationships\n",
    "# graph.query(\n",
    "#     \"\"\"\n",
    "# MERGE (m:Movie {name:\"Top Gun\", runtime: 120})\n",
    "# WITH m\n",
    "# UNWIND [\"Tom Cruise\", \"Val Kilmer\", \"Anthony Edwards\", \"Meg Ryan\"] AS actor\n",
    "# MERGE (a:Actor {name:actor})\n",
    "# MERGE (a)-[:ACTED_IN]->(m)\n",
    "# \"\"\"\n",
    "# )\n",
    "# graph.refresh_schema()\n",
    "# print(graph.schema)\n",
    "# chain = GraphCypherQAChain.from_llm(\n",
    "#     llm=llm, graph=graph, verbose=True, allow_dangerous_requests=True\n",
    "# )\n",
    "# chain.invoke({\"query\": \"Who played in Top Gun?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load structured data into neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "from notebooks.utils import get_clean_job_profiles_df\n",
    "\n",
    "\n",
    "df = get_clean_job_profiles_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_job_profiles_to_neo4j(df, uri, username, password):\n",
    "    \"\"\"\n",
    "    Load job profiles data from DataFrame to Neo4j\n",
    "    \n",
    "    Parameters:\n",
    "    df: pandas DataFrame containing job profiles\n",
    "    uri: Neo4j connection URI (e.g., \"neo4j://localhost:7687\")\n",
    "    username: Neo4j username\n",
    "    password: Neo4j password\n",
    "    \"\"\"\n",
    "    # Connect to Neo4j\n",
    "    driver = GraphDatabase.driver(uri, auth=(username, password))\n",
    "    \n",
    "    with driver.session() as session:\n",
    "        print(\"Deleting all existing data...\")\n",
    "        session.run(\"MATCH (n) DETACH DELETE n\")\n",
    "        \n",
    "        # Create constraints and indexes for better performance\n",
    "        session.run(\"CREATE CONSTRAINT job_profile_id IF NOT EXISTS FOR (j:JobProfile) REQUIRE j.id IS UNIQUE\")\n",
    "        session.run(\"CREATE INDEX job_profile_title IF NOT EXISTS FOR (j:JobProfile) ON (j.title)\")\n",
    "        \n",
    "        # Create indexes for related nodes\n",
    "        for label in [\"BehavioralCompetency\", \"Accountability\", \"Education\", \"Experience\", \n",
    "                     \"Registration\", \"Preference\", \"KSA\", \"WillingnessStatement\", \n",
    "                     \"OptionalRequirement\", \"SecurityScreening\", \"Role\", \"RoleType\", \n",
    "                     \"Classification\", \"Organization\", \"Scope\", \"JobFamily\", \"Stream\", \"ReportsTo\"]:\n",
    "            session.run(f\"CREATE INDEX {label.lower()}_name IF NOT EXISTS FOR (n:{label}) ON (n.name)\")\n",
    "        \n",
    "        # Process each job profile\n",
    "        # i=0\n",
    "        for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Loading job profiles\"):\n",
    "            # i+=1\n",
    "            # if i>10:\n",
    "            #     break\n",
    "            # print('processing: ', i, '/', len(df))\n",
    "            \n",
    "            # Create job profile node\n",
    "            create_job_profile_query = \"\"\"\n",
    "            MERGE (j:JobProfile {id: $id})\n",
    "            SET j.version = $version,\n",
    "                j.title = $title,\n",
    "                j.number = $number,\n",
    "                j.overview = $overview,\n",
    "                j.program_overview = $program_overview,\n",
    "                j.state = $state,\n",
    "                j.type = $type,\n",
    "                j.context = $context,\n",
    "                j.is_archived = $is_archived,\n",
    "                j.all_reports_to = $all_reports_to,\n",
    "                j.valid_from = datetime($valid_from),\n",
    "                j.valid_to = CASE WHEN $valid_to IS NULL THEN NULL ELSE datetime($valid_to) END,\n",
    "                j.views = $views,\n",
    "                j.created_at = datetime($created_at),\n",
    "                j.updated_at = datetime($updated_at),\n",
    "                j.published_at = CASE WHEN $published_at IS NULL THEN NULL ELSE datetime($published_at) END\n",
    "            RETURN j\n",
    "            \"\"\"\n",
    "            \n",
    "            # Convert timestamps to ISO format for Neo4j\n",
    "            valid_from = row['valid_from'].isoformat() if pd.notna(row.get('valid_from')) else None\n",
    "            valid_to = row['valid_to'].isoformat() if pd.notna(row.get('valid_to')) else None\n",
    "            created_at = row['created_at'].isoformat() if pd.notna(row.get('created_at')) else None\n",
    "            updated_at = row['updated_at'].isoformat() if pd.notna(row.get('updated_at')) else None\n",
    "            published_at = row['published_at'].isoformat() if pd.notna(row.get('published_at')) else None\n",
    "            \n",
    "            # Create job profile node\n",
    "            job_profile = session.run(\n",
    "                create_job_profile_query,\n",
    "                id=int(row['id']),\n",
    "                version=int(row['version']) if pd.notna(row.get('version')) else None,\n",
    "                title=row['title'] if pd.notna(row.get('title')) else None,\n",
    "                number=int(row['number']) if pd.notna(row.get('number')) else None,\n",
    "                overview=row['overview'] if pd.notna(row.get('overview')) else None,\n",
    "                program_overview=row['program_overview'] if pd.notna(row.get('program_overview')) else None,\n",
    "                state=row['state'] if pd.notna(row.get('state')) else None,\n",
    "                type=row['type'] if pd.notna(row.get('type')) else None,\n",
    "                context=row['context'] if pd.notna(row.get('context')) else None,\n",
    "                is_archived=bool(row['is_archived']) if pd.notna(row.get('is_archived')) else False,\n",
    "                all_reports_to=bool(row['all_reports_to']) if pd.notna(row.get('all_reports_to')) else False,\n",
    "                valid_from=valid_from,\n",
    "                valid_to=valid_to,\n",
    "                views=int(row['views']) if pd.notna(row.get('views')) else 0,\n",
    "                created_at=created_at,\n",
    "                updated_at=updated_at,\n",
    "                published_at=published_at\n",
    "            ).single()\n",
    "            \n",
    "            # Create relationships for list fields\n",
    "            create_relationships(session, row['id'], 'behavioural_competencies', 'BehavioralCompetency', 'HAS_COMPETENCY', row)\n",
    "            create_relationships(session, row['id'], 'accountabilities', 'Accountability', 'HAS_ACCOUNTABILITY', row)\n",
    "            create_relationships(session, row['id'], 'education', 'Education', 'REQUIRES_EDUCATION', row)\n",
    "            create_relationships(session, row['id'], 'job_experience', 'Experience', 'REQUIRES_EXPERIENCE', row)\n",
    "            create_relationships(session, row['id'], 'professional_registration_requirements', 'Registration', 'REQUIRES_REGISTRATION', row)\n",
    "            create_relationships(session, row['id'], 'preferences', 'Preference', 'HAS_PREFERENCE', row)\n",
    "            create_relationships(session, row['id'], 'knowledge_skills_abilities', 'KSA', 'REQUIRES_KSA', row)\n",
    "            create_relationships(session, row['id'], 'willingness_statements', 'WillingnessStatement', 'HAS_WILLINGNESS', row)\n",
    "            create_relationships(session, row['id'], 'optional_requirements', 'OptionalRequirement', 'HAS_OPTIONAL_REQUIREMENT', row)\n",
    "            create_relationships(session, row['id'], 'security_screenings', 'SecurityScreening', 'REQUIRES_SCREENING', row)\n",
    "            create_relationships(session, row['id'], 'role', 'Role', 'HAS_ROLE', row)\n",
    "            create_relationships(session, row['id'], 'role_type', 'RoleType', 'HAS_ROLE_TYPE', row)\n",
    "            create_relationships(session, row['id'], 'classifications', 'Classification', 'HAS_CLASSIFICATION', row)\n",
    "            create_relationships(session, row['id'], 'organizations', 'Organization', 'BELONGS_TO_ORGANIZATION', row)\n",
    "            create_relationships(session, row['id'], 'scopes', 'Scope', 'HAS_SCOPE', row)\n",
    "            create_relationships(session, row['id'], 'job_families', 'JobFamily', 'BELONGS_TO_JOB_FAMILY', row)\n",
    "            create_relationships(session, row['id'], 'streams', 'Stream', 'BELONGS_TO_STREAM', row)\n",
    "            create_relationships(session, row['id'], 'reports_to', 'ReportsTo', 'REPORTS_TO', row)\n",
    "    \n",
    "    driver.close()\n",
    "    print(\"Data loading completed successfully!\")\n",
    "\n",
    "def create_relationships(session, job_id, field_name, node_label, relationship_type, row):\n",
    "    \"\"\"Create relationships between job profile and related entities\"\"\"\n",
    "    if isinstance(row.get(field_name), list) and len(row.get(field_name)) > 0:\n",
    "        for item in row[field_name]:\n",
    "            query = f\"\"\"\n",
    "            MATCH (j:JobProfile {{id: $job_id}})\n",
    "            MERGE (n:{node_label} {{name: $name}})\n",
    "            MERGE (j)-[r:{relationship_type}]->(n)\n",
    "            RETURN j, n\n",
    "            \"\"\"\n",
    "            session.run(query, job_id=int(job_id), name=item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URI = \"neo4j://localhost:7687\"\n",
    "USERNAME = \"neo4j\"\n",
    "PASSWORD = \"your_password\"\n",
    "\n",
    "# Load data to Neo4j\n",
    "load_job_profiles_to_neo4j(df, URI, USERNAME, PASSWORD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform documents to graph objects with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processed up to:\n",
    "# Inserted 1 graph elements from batch 188\n",
    "# Processing document batch 189/1130\n",
    "# Clear existing data\n",
    "graph.query(\"MATCH (n) DETACH DELETE n\")\n",
    "\n",
    "if not RUN_IN_BATCH:\n",
    "    graph_documents = debug_transformer.convert_to_graph_documents(chunks)\n",
    "    print(graph_documents)\n",
    "\n",
    "    # Load your GraphDocument data\n",
    "    graph.add_graph_documents(\n",
    "        graph_documents,\n",
    "        baseEntityLabel=True,    # Adds __Entity__ label for better indexing\n",
    "        include_source=True      # Maintains document source relationships\n",
    "    )\n",
    "else:\n",
    "    # Process documents in batches\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"Processing document batch {i+1}/{len(chunks)}\")\n",
    "        \n",
    "        # Convert single chunk to graph document\n",
    "        graph_doc = debug_transformer.convert_to_graph_documents([chunk])  # Wrap in list\n",
    "        \n",
    "        # Add to database immediately\n",
    "        graph.add_graph_documents(\n",
    "            graph_doc,\n",
    "            baseEntityLabel=True,\n",
    "            include_source=True\n",
    "        )\n",
    "        \n",
    "        # Optional: Add progress tracking\n",
    "        print(f\"Inserted {len(graph_doc)} graph elements from batch {i+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.query(\"CALL db.schema.visualization()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.refresh_schema()\n",
    "print(graph.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check relationship count in database\n",
    "result = graph.query(\"\"\"\n",
    "    MATCH ()-[r]->() \n",
    "    RETURN count(r) AS relationship_count,\n",
    "           collect(distinct type(r)) AS relationship_types\n",
    "\"\"\")\n",
    "print(f\"Relationships Found: {result[0]['relationship_count']}\")\n",
    "print(f\"Relationship Types: {result[0]['relationship_types']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all Jobprofiles and their organizations\n",
    "result = graph.query(\"MATCH (jp:Jobprofile)-[:BELONGS_TO_ORGANIZATION]->(org:Organization) RETURN jp.id, org.id\")\n",
    "print(result)\n",
    "print('========')\n",
    "# Get all Documents mentioning Licensing Clerk\n",
    "print(graph.query(\"MATCH (d:Document {title: 'Licensing Clerk'})-[:MENTIONS]->(jp:Jobprofile) RETURN d.title, jp.id\"))\n",
    "print('====== ai not workign: ')\n",
    "print(graph.query(\"MATCH (jp:Jobprofile {title: 'Licensing Clerk'})-[:BELONGS_TO_ORGANIZATION]->(o:Organization) RETURN o.id\")) # AI generated - not wokring - confused title with id\n",
    "print('====== same prompt but claude: ')\n",
    "print(graph.query(\"MATCH (j:Jobprofile)-[:BELONGS_TO_ORGANIZATION]->(o:Organization) WHERE j.id = 'Licensing Clerk' RETURN o.id\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This generates a cypher for exact querying of the neo4j database (as opposed to using vector index for similarity search):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = GraphCypherQAChain.from_llm(\n",
    "    llm=llm, graph=graph, verbose=True, allow_dangerous_requests=True,\n",
    "    # exclude_types=['Document']\n",
    "    # validate_cypher=True,  # New critical parameter\n",
    "    # schema_constraints={\n",
    "    #     \"Jobprofile\": {\"identifier\": \"id\"},  # Force 'id' usage\n",
    "    #     \"Document\": {\"identifier\": \"title\"}\n",
    "    # }\n",
    ")\n",
    "chain.invoke({\"query\": \"What organizations does the 'Licensing Clerk' profile belong to? Ensure title is treated as 'id' instead of 'title'\"})\n",
    "# use backticks for labels containing spaces: e.g. MATCH (jt:`Job Title` ========== \\n "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Visualization with Pyvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %% [code]\n",
    "# from pyvis.network import Network\n",
    "\n",
    "# # Initialize network with configuration\n",
    "# net = Network(\n",
    "#     notebook=True, \n",
    "#     cdn_resources=\"in_line\", \n",
    "#     height=\"750px\"\n",
    "# )\n",
    "\n",
    "# # Add nodes with metadata\n",
    "# nodes = graph.query(\"\"\"\n",
    "#     MATCH (n) \n",
    "#     RETURN n.id as id, \n",
    "#            n.name as label, \n",
    "#            n.type as group\n",
    "# \"\"\")\n",
    "\n",
    "# # Process each node and add to network\n",
    "# for node in nodes:\n",
    "#     net.add_node(\n",
    "#         node[\"id\"],\n",
    "#         label=node[\"label\"],\n",
    "#         group=node[\"group\"],\n",
    "#         title=f\"Type: {node['group']}\"\n",
    "#     )\n",
    "\n",
    "# # Add relationships with labels\n",
    "# relationships = graph.query(\"\"\"\n",
    "#     MATCH (s)-[r]->(t) \n",
    "#     RETURN s.id as source, \n",
    "#            t.id as target, \n",
    "#            type(r) as label\n",
    "# \"\"\")\n",
    "\n",
    "# # Process each relationship and add to network\n",
    "# for rel in relationships:\n",
    "#     net.add_edge(\n",
    "#         rel[\"source\"], \n",
    "#         rel[\"target\"],\n",
    "#         label=rel[\"label\"],\n",
    "#         arrowStrikethrough=False\n",
    "#     )\n",
    "\n",
    "# # Generate and save interactive visualization\n",
    "# net.show(\"job_network.html\")\n",
    "\n",
    "# NEW\n",
    "\n",
    "# %% [code]\n",
    "from pyvis.network import Network\n",
    "\n",
    "# Initialize network with optimized configuration\n",
    "net = Network(\n",
    "    notebook=True, \n",
    "    cdn_resources=\"in_line\",\n",
    "    # height=\"750px\",\n",
    "    # width=\"100%\",\n",
    "    # layout={\n",
    "    #     # \"hierarchical\": {\"enabled\": True},\n",
    "    #     # \"levelSeparation\": 150,    # Vertical spacing between levels [5]\n",
    "    #     \"nodeSpacing\": 200,        # Minimum horizontal spacing [5]\n",
    "    #     \"treeSpacing\": 300         # Spacing between disconnected components [5]\n",
    "    # }\n",
    ")\n",
    "\n",
    "\n",
    "# Configure physics system for optimal node distribution [1][4][7]\n",
    "# net.set_options(\"\"\"\n",
    "# {\n",
    "#     \"physics\": {\n",
    "#         \"enabled\": true,\n",
    "#         \"solver\": \"forceAtlas2Based\",\n",
    "#         \"forceAtlas2Based\": {\n",
    "#             \"gravitationalConstant\": -150,\n",
    "#             \"centralGravity\": 0.01,\n",
    "#             \"springLength\": 250,\n",
    "#             \"springConstant\": 0.005,\n",
    "#             \"damping\": 0.4,\n",
    "#             \"avoidOverlap\": 1.0\n",
    "#         },\n",
    "#         \"maxVelocity\": 75,\n",
    "#         \"minVelocity\": 2,\n",
    "#         \"timestep\": 0.5\n",
    "#     }\n",
    "# }\n",
    "# \"\"\")\n",
    "\n",
    "# Add nodes with extended metadata for visual clarity [9]\n",
    "nodes = graph.query(\"\"\"\n",
    "    MATCH (n) \n",
    "    RETURN n.id as id, \n",
    "           n.name as label, \n",
    "           n.type as group\n",
    "\"\"\")\n",
    "\n",
    "for node in nodes:\n",
    "    net.add_node(\n",
    "        node[\"id\"],\n",
    "        label=node[\"label\"],\n",
    "        group=node[\"group\"],\n",
    "        title=f\"\"\"\n",
    "            Type: {node['group']}\n",
    "            Connections: {node.get('degree', 0)}\n",
    "        \"\"\",\n",
    "        value=node.get(\"value\", 10),  # Default size if missing [9]\n",
    "        borderWidth=2,                # Clear node boundaries [9]\n",
    "        shape=\"dot\",                  # Consistent node shape\n",
    "        font={\"size\": 18}             # Improved label readability\n",
    "    )\n",
    "\n",
    "# Add relationships with enhanced visual properties [1][10]\n",
    "relationships = graph.query(\"\"\"\n",
    "    MATCH (s)-[r]->(t) \n",
    "    RETURN s.id as source, \n",
    "           t.id as target, \n",
    "           type(r) as label\n",
    "\"\"\")\n",
    "\n",
    "for rel in relationships:\n",
    "    net.add_edge(\n",
    "        rel[\"source\"], \n",
    "        rel[\"target\"],\n",
    "        label=rel[\"label\"],\n",
    "        value=rel.get(\"value\", 1),    # Default edge weight [10]\n",
    "        smooth={\"type\": \"dynamic\"},   # Curved edge rendering [12]\n",
    "        arrowStrikethrough=False,\n",
    "        # color={\n",
    "        #     \"color\": \"#95a5a6\",       # Base edge color\n",
    "        #     \"highlight\": \"#3498db\"    # Highlight color on hover\n",
    "        # },\n",
    "        # width=2,                      # Visual weight multiplier [10]\n",
    "        physics=True                   # Enable edge spring behavior [9]\n",
    "    )\n",
    "\n",
    "# Final layout optimization steps [2][6]\n",
    "# net.toggle_physics(True)             # Enable for initial stabilization\n",
    "# net.show_buttons(filter_=['physics']) # Allow parameter adjustments [6]\n",
    "\n",
    "net.set_options(\"\"\"\n",
    "{\n",
    "    \"physics\": {\n",
    "        \"enabled\": true,\n",
    "        \"solver\": \"repulsion\",\n",
    "        \"repulsion\": {\n",
    "            \"nodeDistance\": 300,\n",
    "            \"springLength\": 200\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "# Generate and save visualization with preservation options\n",
    "net.write_html(\n",
    "    \"job_network.html\",\n",
    "    local=False,\n",
    "    notebook=False,\n",
    "    # override=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build vector index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all text fields in the neo4j db:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get available text properties across all nodes\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "driver = GraphDatabase.driver(\"bolt://localhost:7687\", \n",
    "                              auth=(\"neo4j\", \"your_password\"))\n",
    "\n",
    "def get_text_properties(driver, min_percentage=0.1):\n",
    "    with driver.session() as session:\n",
    "        result = session.run(\"\"\"\n",
    "            MATCH (n)\n",
    "            WITH count(*) as total\n",
    "            MATCH (n)\n",
    "            UNWIND keys(n) AS prop\n",
    "            WITH prop, count(*) as count, total\n",
    "            WHERE count >= total * $min_percentage\n",
    "            AND apoc.meta.cypher.type(prop) = 'STRING'\n",
    "            RETURN collect(prop) AS props\n",
    "        \"\"\", {\"min_percentage\": min_percentage})\n",
    "        return result.single()[\"props\"]\n",
    "\n",
    "text_props = get_text_properties(driver)\n",
    "print(text_props)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorstore generation from existing graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to existing Neo4jGraph initialization\n",
    "from langchain_community.vectorstores import Neo4jVector\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"thenlper/gte-small\")\n",
    "\n",
    "# internally it does this:\n",
    "# read_query = (\n",
    "#   \"CALL db.index.vector.queryNodes($index, $k, $embedding) \"\n",
    "#   \"YIELD node, score \"\n",
    "# ) + retrieval_query\n",
    "\n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "# his method pulls relevant text information from the database, and calculates and stores the text embeddings back to the database.\n",
    "vector_store = Neo4jVector.from_existing_graph(\n",
    "    embedding=embeddings,\n",
    "    url=os.environ[\"NEO4J_URI\"],\n",
    "    username=os.environ[\"NEO4J_USERNAME\"],\n",
    "    password=os.environ[\"NEO4J_PASSWORD\"],\n",
    "    index_name=\"document_embeddings\",\n",
    "    # Label-Aware Indexing - Treating all nodes as generic entities while preserving original labels in metadata\n",
    "    # node_label=\"Jobprofile\",\n",
    "    # Dynamic Property Concatenation - Combining properties across node types into a single text embedding source\n",
    "    # text_node_properties=[\"text\",\"title\",\"id\"],\n",
    "    embedding_node_property=\"embedding\",\n",
    "    # If the data is unstructured, coalesce\n",
    "    #  OPTIONAL MATCH (node)-[r]->(related) # This is unidirectional\n",
    "\n",
    "    # these first two lines are not neede decause of concat above:\n",
    "    # MATCH (node:Entity)\n",
    "    # WITH node, 1.0 as score\n",
    "    \n",
    "    # \"We know the similarity search query will return the node and score variables, so we can pass those into our retrieval query to pull connected data of those similar nodes.\"\n",
    "    retrieval_query=\"\"\"\n",
    "CALL db.index.vector.queryNodes($index, $k, $embedding) \n",
    "    YIELD node AS vectorNode, score AS similarityScore  // Alias both variables\n",
    "    \n",
    "    WITH vectorNode, similarityScore \n",
    "    ORDER BY similarityScore DESC LIMIT 5\n",
    "    \n",
    "    WITH vectorNode,\n",
    "         apoc.map.clean(vectorNode {.*}, ['embedding'], []) AS nodeProps,\n",
    "         labels(vectorNode) AS nodeLabels,\n",
    "         similarityScore\n",
    "    \n",
    "    OPTIONAL MATCH (vectorNode)-[r:PART_OF]-(related)\n",
    "    \n",
    "    WITH nodeProps, nodeLabels, similarityScore,\n",
    "         collect(DISTINCT { \n",
    "           rel: type(r), \n",
    "           target: related {.*, labels: labels(related)} \n",
    "         }) AS connections,\n",
    "         [x IN collect(coalesce(related.description, related.name, '')) WHERE x <> ''] AS relatedContext\n",
    "    \n",
    "    RETURN apoc.text.join(\n",
    "        [coalesce(nodeProps.text, nodeProps.title, nodeProps.id, '')] + relatedContext,\n",
    "        ' | '\n",
    "    ) AS text,\n",
    "    \n",
    "    similarityScore AS score,  // Rename back to expected 'score' field\n",
    "    \n",
    "    { \n",
    "      source: apoc.map.merge(nodeProps, {labels: nodeLabels}),\n",
    "      connections: connections \n",
    "    } AS metadata\n",
    "\"\"\",\n",
    "node_label=\"Entity\", # Generic label for all nodes\n",
    "text_node_properties=text_props #\"name\", \"description\", \"title\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension, entity_type = vector_store.retrieve_existing_index()\n",
    "print('dimension: ', dimension, ', entity_type: ', entity_type)\n",
    "\n",
    "sample_data = vector_store.query(\"\"\"\n",
    "MATCH (n:`Job Title`)\n",
    "RETURN n.id as id,\n",
    "       size(n.embedding) as embedding_size\n",
    "LIMIT 5\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_info = vector_store.query(\"\"\"\n",
    "SHOW INDEXES \n",
    "WHERE name = 'document_embeddings'\n",
    "\"\"\")\n",
    "print(index_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_embedding = embeddings.embed_query(\"test\")\n",
    "print(len(sample_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create optimized vector index\n",
    "# # todo: is this needed?\n",
    "# graph.query(\"\"\"\n",
    "# CREATE VECTOR INDEX document_embeddings IF NOT EXISTS\n",
    "# FOR (n:Document) ON (n.embedding)\n",
    "# OPTIONS {\n",
    "#   indexConfig: {\n",
    "#     `vector.dimensions`: 384,\n",
    "#     `vector.similarity_function`: 'cosine'\n",
    "#   }\n",
    "# }\n",
    "# \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with driver.session() as session:\n",
    "    traversal_plan = session.run(\"\"\"\n",
    "        EXPLAIN \n",
    "        MATCH (node:Entity)\n",
    "        WITH node, 1.0 as score\n",
    "        OPTIONAL MATCH (node)-[r]->(related)\n",
    "        RETURN apoc.text.join([\n",
    "            coalesce(node.text, node.name, node.title),\n",
    "            coalesce(related.name, related.title)\n",
    "        ], ' ') AS text,\n",
    "        score,\n",
    "        node {.*, labels: labels(node)} AS metadata,\n",
    "        collect({relationship: type(r), node: related}) AS graph_context\n",
    "    \"\"\").consume()\n",
    "    \n",
    "    print(\"Query execution plan:\")\n",
    "    print(traversal_plan.plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "\n",
    "results = vector_store.similarity_search(\"Public Safety & Sol General (PSSG)\", k=10)\n",
    "for doc in results:\n",
    "    print('==== DOC ====')\n",
    "    pprint(doc.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_results = []\n",
    "for doc in results:\n",
    "    debug_entry = {\n",
    "        \"text\": doc.page_content,\n",
    "        # \"score\": doc.metadata['score'],\n",
    "        \"source_node\": {\n",
    "            \"id\": doc.metadata['id'],\n",
    "            \"labels\": doc.metadata['labels'],\n",
    "            \"properties\": {k:v for k,v in doc.metadata.items() \n",
    "                          if k not in ['score', 'id', 'labels']}\n",
    "        },\n",
    "        \"connections\": [\n",
    "            {\n",
    "                \"relationship\": rel['relationship'],\n",
    "                \"target_node\": {\n",
    "                    \"id\": rel['node'].get('id'),\n",
    "                    \"labels\": rel['node'].get('labels', []),\n",
    "                    \"properties\": {k:v for k,v in rel['node'].items()\n",
    "                                  if k not in ['id', 'labels']}\n",
    "                }\n",
    "            } for rel in doc.metadata.get('graph_context', [])\n",
    "        ]\n",
    "    }\n",
    "    debug_results.append(debug_entry)\n",
    "\n",
    "# 3. Analyze the traversal patterns\n",
    "print(\"\\nTraversal statistics:\")\n",
    "for doc in debug_results:\n",
    "    print(f\"Node {doc['source_node']['id']} ({'|'.join(doc['source_node']['labels'])})\")\n",
    "    print(f\"Connected to {len(doc['connections'])} nodes via:\")\n",
    "    for conn in doc['connections']:\n",
    "        print(f\"  - {conn['relationship']} → \"\n",
    "              f\"{conn['target_node']['labels']} \"\n",
    "              f\"(props: {list(conn['target_node']['properties'].keys())})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View all relationships\n",
    "with driver.session() as session:\n",
    "    result = session.run(\"\"\"\n",
    "        MATCH (n:Entity)-[r]->() \n",
    "        RETURN type(r) AS rel_type, count(*) AS count\n",
    "        UNION\n",
    "        MATCH (n:Entity)<-[r]-()\n",
    "        RETURN type(r) AS rel_type, count(*) AS count\n",
    "    \"\"\").data()\n",
    "    \n",
    "    print(\"Relationship counts:\")\n",
    "    for record in result:\n",
    "        print(f\"{record['rel_type']}: {record['count']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with driver.session() as session:\n",
    "    label_check = session.run(\"\"\"\n",
    "        MATCH (n)\n",
    "        WHERE NOT 'Entity' IN labels(n)\n",
    "        RETURN count(*) AS missing_label_count\n",
    "    \"\"\").single()\n",
    "    \n",
    "    print(f\"Nodes missing Entity label: {label_check['missing_label_count']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vector_store.query(\"\"\"\n",
    "MATCH (n:Entity)\n",
    "RETURN n {.*} as node\n",
    "\"\"\")\n",
    "print(results[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "\n",
    "results = vector_store.similarity_search(\"Concern for order Perf and Risk Mgt Analyst\", k=2)\n",
    "for doc in results:\n",
    "    print('==== DOC ====')\n",
    "    pprint(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vector_store.query(\"\"\"\n",
    "MATCH (node:Entity)\n",
    "    WITH node, 1.0 as score\n",
    "    OPTIONAL MATCH (node)-[r]-(related)\n",
    "    RETURN apoc.text.join([\n",
    "        coalesce(node.text, node.name, node.title, node.id),\n",
    "        coalesce(related.name, related.title, node.id)\n",
    "    ], ' ') AS text,\n",
    "    score,\n",
    "    node {.*, labels: labels(node)} AS metadata,\n",
    "    collect({relationship: type(r), node: related}) AS graph_context\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
